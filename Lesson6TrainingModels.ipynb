{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson6TrainingModels.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n5aPnq_DMD7Q",
        "rE4eSb9wMTjp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbechara/HertieML/blob/main/Lesson6TrainingModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUz_uS-hblnS"
      },
      "source": [
        "# Implement Batch Gradient Descent with early stopping (optional) for Softmax Regression (without using Scikit Learn)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5aPnq_DMD7Q"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkCiS4dW4smV"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"training_linear_models\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "np.random.seed(2042)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4eSb9wMTjp"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT2PEH1XMQ48",
        "outputId": "c5de3339-b25a-48fb-9af1-d72bbe0aecf7"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3WAZ_BoOVk9"
      },
      "source": [
        "# What are the steps we will need to take to implement this model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwnVaupHV-8_"
      },
      "source": [
        "## Problem 1: Data\n",
        "\n",
        " - Split the Training set. Either use scikit learn's built-in split function or write your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG8TrN_Gmkyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903dcfa9-6cd2-497f-ab93-ed6a6e4b53e1"
      },
      "source": [
        "# Thilos Solution\n",
        "# First: Extract the relevant data\n",
        "# From the description Petal length and width seem to be highly correlated so let's extract these as the features\n",
        "X = iris[\"data\"][:,(2,3)]\n",
        "y = iris[\"target\"]\n",
        "y.shape\n",
        "# Ok we have 150 instances, so it is a really small data set.\n",
        "\n",
        "# Let's split the data set.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
        "\n",
        "print(len(y_train), y_train.shape)\n",
        "\n",
        "# OneHotEncoder (manually)\n",
        "# 1st way: Simple if/for loop\n",
        "\n",
        "def y_converter(input_vector):\n",
        "  y_converted = []\n",
        "  for i in range(len(input_vector)):\n",
        "    if input_vector[i] == 0:\n",
        "      y_converted.append([1, 0, 0])\n",
        "    elif input_vector[i] == 1:\n",
        "      y_converted.append([0, 1, 0])\n",
        "    elif input_vector[i] == 2:\n",
        "      y_converted.append([0, 0, 1])\n",
        "    else:\n",
        "      print(\"Error! Category out of anticipated range...\")\n",
        "  return y_converted\n",
        "\n",
        "# Call the function\n",
        "y_train_conv = y_converter(y_train)\n",
        "# Print\n",
        "print(y_train, y_train_conv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90 (90,)\n",
            "[2 0 0 0 1 0 1 2 0 1 2 0 2 2 1 1 2 1 0 1 2 0 0 1 1 0 2 0 0 1 1 2 1 2 2 1 0\n",
            " 0 2 2 0 0 0 1 2 0 2 2 0 1 1 2 1 2 0 2 1 2 1 1 1 0 1 1 0 1 2 2 0 1 2 2 0 2\n",
            " 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2] [[0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojd4AtkZWLEC"
      },
      "source": [
        "- Remember: Softmax outputs a vector that represents the probability distributions of a list of potential outcomes.\n",
        "\n",
        "- So write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. \n",
        "\n",
        "0 -> [1,0,0]\n",
        "\n",
        "1 -> [0,1,0]\n",
        "\n",
        "2 -> [0,0,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hc4q06YtIIG"
      },
      "source": [
        "Book Solution for data splitting and encoding.\n",
        "Note: this includes an additional function for bias, which we did not have time to cover in class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEvJXgNFczFp"
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
        "\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzIx-VIwt3kT"
      },
      "source": [
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgFRPlKTt6jW"
      },
      "source": [
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCusZAeNWU8k"
      },
      "source": [
        "## Problem 2: Softmax\n",
        "\n",
        "Tip 1: \n",
        "This is the equation that defines softmax:\n",
        "\n",
        "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$\n",
        "\n",
        "Tip 2: You can find the code somewhere on stack overflow and adapt it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ2U0f8oqE8O"
      },
      "source": [
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8m-QpczXVhi"
      },
      "source": [
        "## Problem 3: Training.\n",
        "\n",
        "- To train you need to model the loss and the gradient. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9bw-r8Raort"
      },
      "source": [
        "The cost function:\n",
        "\n",
        "$J(\\mathbf{\\Theta}) =\n",
        "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PCg5-NmasHZ"
      },
      "source": [
        "The gradients:\n",
        "\n",
        "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg3eSYm4WK5d"
      },
      "source": [
        "I have provided the code for you here, so you don't need to write the training loop if you implemented the softmax function correctly.\n",
        "\n",
        "\n",
        "All I want you to do now is to add early stopping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsL4MSoIr9_I"
      },
      "source": [
        "# because we're using softmax and therefore a one-hot vector, we need to \n",
        "# make sure we randomly initialise Theta with the correct dimensions\n",
        "\n",
        "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
        "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbFA2IeXuKba"
      },
      "source": [
        "def loss_function(Y_train_one_hot):\n",
        "  return -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEauHc9WV0Y6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad07f351-49a4-484f-b700-16849deedd5e"
      },
      "source": [
        "# And here is the code for batch gradient descent with softmax\n",
        "\n",
        "eta = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "# we start by randomising theta \n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    # converts the list to a probability list using softmax \n",
        "    Y_proba = softmax(logits) \n",
        "    if iteration % 500 == 0:\n",
        "        # shows the loss every 500 iterations\n",
        "        loss = loss_function(Y_train_one_hot)\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/len(X_train) * X_train.T.dot(error)\n",
        "    # Update Theta\n",
        "    Theta = Theta - eta * gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5.173284880908112\n",
            "500 0.8258143504756522\n",
            "1000 0.6740383508681776\n",
            "1500 0.5891518016822946\n",
            "2000 0.5353052890403674\n",
            "2500 0.4975988211901051\n",
            "3000 0.469220320068328\n",
            "3500 0.4467104744290491\n",
            "4000 0.4281482798645294\n",
            "4500 0.41238656131534807\n",
            "5000 0.3986986115898958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mIUTkUXxYjW"
      },
      "source": [
        "# Thilos solution\n",
        "# And here is the code for batch gradient descent with softmax\n",
        "\n",
        "eta = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "# we start by randomising theta \n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "#Defining the loss function\n",
        "def loss_function(Y_train_one_hot):\n",
        "  return -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "\n",
        "# Defining smallest loss\n",
        "smallest_loss = np.infty\n",
        "\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    # converts the list to a probability list using softmax \n",
        "    Y_proba = softmax(logits)\n",
        "    # loss = loss_function(Y_train_one_hot) Not calculate it on every step, as it is costly.\n",
        "    if iteration % 500 == 0:\n",
        "        # shows the loss every 500 iterations\n",
        "        loss = loss_function(Y_train_one_hot)\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/len(X_train) * X_train.T.dot(error)\n",
        "    # Update Theta\n",
        "    Theta = Theta - eta * gradients\n",
        "    # Early stop 1\n",
        "    loss_diff = smallest_loss - loss\n",
        "    if loss_diff < epsilon:\n",
        "      print(\"Loss improvement is super small! --> Entering early stopping\")\n",
        "      print(\"Loss improvement in this iteration: \", loss_diff)\n",
        "      print(\"Threshold was epsilon: \", epsilon)\n",
        "      break\n",
        "    # Early stop 2\n",
        "    if loss < smallest_loss:\n",
        "      smallest_loss = loss\n",
        "    else:\n",
        "      print(\"Loss is getting bigger again! --> Entering early stopping\")\n",
        "      print(\"Loss of iteration before: \", iteration-1, smallest_loss)\n",
        "      print(\"Loss of current iteration: \", iteration, loss)\n",
        "      break\n",
        "    # Early stop 1\n",
        "\n",
        "    if loss < epsilon:\n",
        "      print(\"Loss improvement is super small! --> Entering early stopping\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LAyf7xRa1pC"
      },
      "source": [
        "## Problem 4: Testing\n",
        "\n",
        "- Ok we're almost done, now let's try some predictions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdL2XxZtbOXJ"
      },
      "source": [
        "- Now let's measure the accuracy on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxcXjw9rbSqZ"
      },
      "source": [
        "In case we don't manage to go through it all, I've put the solutions in another colab [file](https://colab.research.google.com/drive/1_tblkukSgl50g9Od9dQ-ugNh9WTppjLE?usp=sharing): "
      ]
    }
  ]
}